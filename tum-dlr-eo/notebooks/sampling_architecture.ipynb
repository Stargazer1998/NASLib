{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "device: cpu\n",
      "device: cpu\n",
      "device: cpu\n",
      "device: cpu\n",
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "from naslib.defaults.trainer import Trainer\n",
    "\n",
    "from naslib.search_spaces import (\n",
    "    DartsSearchSpace,\n",
    "    SimpleCellSearchSpace,\n",
    "    NasBench101SearchSpace,\n",
    "    HierarchicalSearchSpace,\n",
    ")\n",
    "from naslib.search_spaces.nasbench101 import graph\n",
    "\n",
    "from naslib.utils import get_dataset_api, setup_logger, utils\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.get_config_from_args(config_type=\"nas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from file... This may take a few minutes...\n",
      "Loaded dataset in 5 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset_api = get_dataset_api(config.search_space, config.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = \"input\"\n",
    "OUTPUT = \"output\"\n",
    "CONV3X3 = \"conv3x3-bn-relu\"\n",
    "CONV1X1 = \"conv1x1-bn-relu\"\n",
    "MAXPOOL3X3 = \"maxpool3x3\"\n",
    "OPS = [CONV3X3, CONV1X1, MAXPOOL3X3]\n",
    "\n",
    "NUM_VERTICES = 7\n",
    "OP_SPOTS = NUM_VERTICES - 2\n",
    "MAX_EDGES = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_architecture(dataset_api, arch_limit=10):\n",
    "        \"\"\"\n",
    "        This will sample a random architecture and update the edges in the\n",
    "        naslib object accordingly.\n",
    "        From the NASBench repository:\n",
    "        one-hot adjacency matrix\n",
    "        draw [0,1] for each slot in the adjacency matrix\n",
    "        \"\"\"\n",
    "        architectures = []\n",
    "        while len(architectures) < arch_limit:\n",
    "            matrix = np.random.choice([0, 1], size=(NUM_VERTICES, NUM_VERTICES))\n",
    "            matrix = np.triu(matrix, 1)\n",
    "            ops = np.random.choice(OPS, size=NUM_VERTICES).tolist()\n",
    "            ops[0] = INPUT\n",
    "            ops[-1] = OUTPUT\n",
    "            spec = dataset_api[\"api\"].ModelSpec(matrix=matrix, ops=ops)\n",
    "            if dataset_api[\"nb101_data\"].is_valid(spec):\n",
    "                architectures.append({\"matrix\": matrix, \"ops\": ops})\n",
    "                #break\n",
    "        \n",
    "        return architectures\n",
    "            \n",
    "        #self.set_spec({\"matrix\": matrix, \"ops\": ops})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_architectures = sample_random_architecture(dataset_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'matrix': array([[0, 0, 0, 1, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 1, 0],\n",
       "         [0, 0, 0, 0, 1, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'maxpool3x3',\n",
       "   'conv1x1-bn-relu',\n",
       "   'maxpool3x3',\n",
       "   'conv1x1-bn-relu',\n",
       "   'maxpool3x3',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 1, 1, 1, 1, 1, 0],\n",
       "         [0, 0, 1, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'maxpool3x3',\n",
       "   'maxpool3x3',\n",
       "   'conv3x3-bn-relu',\n",
       "   'conv3x3-bn-relu',\n",
       "   'maxpool3x3',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 0, 1, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 1, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 1, 1, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'maxpool3x3',\n",
       "   'conv1x1-bn-relu',\n",
       "   'maxpool3x3',\n",
       "   'conv3x3-bn-relu',\n",
       "   'conv3x3-bn-relu',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 1, 1, 1, 0, 0, 1],\n",
       "         [0, 0, 1, 1, 0, 1, 1],\n",
       "         [0, 0, 0, 1, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'maxpool3x3',\n",
       "   'conv1x1-bn-relu',\n",
       "   'conv1x1-bn-relu',\n",
       "   'maxpool3x3',\n",
       "   'conv3x3-bn-relu',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 0, 1, 1, 1, 0, 1],\n",
       "         [0, 0, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 1, 1, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'maxpool3x3',\n",
       "   'maxpool3x3',\n",
       "   'conv3x3-bn-relu',\n",
       "   'conv3x3-bn-relu',\n",
       "   'maxpool3x3',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 0, 1, 0, 1, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'conv3x3-bn-relu',\n",
       "   'conv1x1-bn-relu',\n",
       "   'conv3x3-bn-relu',\n",
       "   'conv3x3-bn-relu',\n",
       "   'conv3x3-bn-relu',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 1, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 1, 0, 1, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'maxpool3x3',\n",
       "   'maxpool3x3',\n",
       "   'conv1x1-bn-relu',\n",
       "   'maxpool3x3',\n",
       "   'conv3x3-bn-relu',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 1, 0, 0, 1, 0, 1],\n",
       "         [0, 0, 0, 1, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'maxpool3x3',\n",
       "   'maxpool3x3',\n",
       "   'maxpool3x3',\n",
       "   'conv3x3-bn-relu',\n",
       "   'conv3x3-bn-relu',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 0, 1, 1, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 1, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'conv1x1-bn-relu',\n",
       "   'maxpool3x3',\n",
       "   'maxpool3x3',\n",
       "   'maxpool3x3',\n",
       "   'conv3x3-bn-relu',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 1, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'conv1x1-bn-relu',\n",
       "   'maxpool3x3',\n",
       "   'conv3x3-bn-relu',\n",
       "   'conv1x1-bn-relu',\n",
       "   'conv1x1-bn-relu',\n",
       "   'output']}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matrix': array([[0, 0, 0, 1, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'ops': ['input',\n",
       "  'maxpool3x3',\n",
       "  'conv1x1-bn-relu',\n",
       "  'maxpool3x3',\n",
       "  'conv1x1-bn-relu',\n",
       "  'maxpool3x3',\n",
       "  'output']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_architectures[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'parse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14296/3356005674.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msampled_architectures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'parse'"
     ]
    }
   ],
   "source": [
    "sampled_architectures[0].parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from naslib.predictors.utils.models import nasbench1 as nas101_arch\n",
    "from naslib.predictors.utils.models import nasbench1_spec\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_project_root() -> Path:\n",
    "    \"\"\"\n",
    "    Returns the root path of the project.\n",
    "    \"\"\"\n",
    "    return Path('./').parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = nasbench1_spec._ToModelSpec(\n",
    "    sampled_architectures[0][\"matrix\"], sampled_architectures[0][\"ops\"]\n",
    ")\n",
    "\n",
    "num_classes_dic = {\"cifar10\": 10, \"cifar100\": 100, \"ImageNet16-120\": 120}\n",
    "\n",
    "network = nas101_arch.Network(\n",
    "    spec,\n",
    "    stem_out=128,\n",
    "    num_stacks=3,\n",
    "    num_mods=3,\n",
    "    num_classes=num_classes_dic[\"cifar10\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "naslib.predictors.utils.models.nasbench1.Network"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_device = network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "naslib.predictors.utils.models.nasbench1.Network"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(on_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutout(object):\n",
    "    def __init__(self, length, prob=1.0):\n",
    "        self.length = length\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if np.random.binomial(1, self.prob):\n",
    "            h, w = img.size(1), img.size(2)\n",
    "            mask = np.ones((h, w), np.float32)\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1:y2, x1:x2] = 0.0\n",
    "            mask = torch.from_numpy(mask)\n",
    "            mask = mask.expand_as(img)\n",
    "            img *= mask\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]\n",
    "CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_transform.transforms.append(Cutout(16, 1))\n",
    "\n",
    "valid_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"{}/data\".format(get_project_root())\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = dset.CIFAR10(\n",
    "    root=data, train=True, download=True, transform=train_transform\n",
    ")\n",
    "test_data = dset.CIFAR10(\n",
    "    root=data, train=False, download=True, transform=valid_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(0.8 * num_train))\n",
    "\n",
    "train_queue = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=100,\n",
    "    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    worker_init_fn=np.random.seed(seed+1),\n",
    ")\n",
    "\n",
    "valid_queue = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=100,\n",
    "    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    worker_init_fn=np.random.seed(seed),\n",
    ")\n",
    "\n",
    "test_queue = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    worker_init_fn=np.random.seed(seed),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from naslib.optimizers import (\n",
    "    DARTSOptimizer,\n",
    "    GDASOptimizer,\n",
    "    DrNASOptimizer,\n",
    "    RandomSearch,\n",
    "    RegularizedEvolution,\n",
    "    LocalSearch,\n",
    "    BasePredictor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RandomSearch(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = NasBench101SearchSpace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.adapt_search_space(search_space, dataset_api=dataset_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = optimizer.new_epoch(108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Module(\n",
       "  (arch): Graph makrograph-0.0963693, scope macro, 6 nodes\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Module(\n",
       "   (arch): Graph makrograph-0.0963693, scope macro, 6 nodes\n",
       " )]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.sampled_archs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeView((1, 2, 3, 4, 5, 6))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_space.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14296/3168881563.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mneigbor_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msearch_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0medge_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_edge_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigbor_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0medge_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0medge_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedded_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Graph' is not defined"
     ]
    }
   ],
   "source": [
    "for node_idx in lexicographical_topological_sort(search_space): \n",
    "    if \"subgraph\" in search_space.nodes[node_idx]:\n",
    "                search_space.nodes[node_idx][\"subgraph\"].parse()\n",
    "                search_space.add_module(\n",
    "                    \"{}-subgraph_at({})\".format(search_space.name, node_idx),\n",
    "                    search_space.nodes[node_idx][\"subgraph\"],\n",
    "                )\n",
    "    else:\n",
    "        if isinstance(search_space.nodes[node_idx][\"comb_op\"], torch.nn.Module):\n",
    "            search_space.add_module(\n",
    "                \"{}-comb_op_at({})\".format(search_space.name, node_idx),\n",
    "                search_space.nodes[node_idx][\"comb_op\"],\n",
    "            )\n",
    "            \n",
    "    for neigbor_idx in search_space.neighbors(node_idx):\n",
    "            edge_data = search_space.get_edge_data(node_idx, neigbor_idx)\n",
    "            if isinstance(edge_data.op, Graph):\n",
    "                edge_data.op.parse()\n",
    "            elif edge_data.op.get_embedded_ops():\n",
    "                for primitive in edge_data.op.get_embedded_ops():\n",
    "                    if isinstance(primitive, Graph):\n",
    "                        primitive.parse()\n",
    "            search_space.add_module(\n",
    "                \"{}-edge({},{})\".format(search_space.name, node_idx, neigbor_idx),\n",
    "                edge_data.op,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.dag import lexicographical_topological_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Module' object has no attribute 'nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14296/215088724.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlexicographical_topological_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampled_archs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"subgraph\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subgraph\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 model.add_module(\n\u001b[1;32m      5\u001b[0m                     \u001b[0;34m\"{}-subgraph_at({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/naslib/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1131\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Module' object has no attribute 'nodes'"
     ]
    }
   ],
   "source": [
    "for node_idx in lexicographical_topological_sort(optimizer.sampled_archs[0].arch):\n",
    "            if \"subgraph\" in model.nodes[node_idx]:\n",
    "                model.nodes[node_idx][\"subgraph\"].parse()\n",
    "                model.add_module(\n",
    "                    \"{}-subgraph_at({})\".format(model.name, node_idx),\n",
    "                    model.nodes[node_idx][\"subgraph\"],\n",
    "                )\n",
    "            else:\n",
    "                if isinstance(model.nodes[node_idx][\"comb_op\"], torch.nn.Module):\n",
    "                    model.add_module(\n",
    "                        \"{}-comb_op_at({})\".format(model.name, node_idx),\n",
    "                        model.nodes[node_idx][\"comb_op\"],\n",
    "                    )\n",
    "            for neigbor_idx in model.neighbors(node_idx):\n",
    "                edge_data = model.get_edge_data(node_idx, neigbor_idx)\n",
    "                edge_data.op.parse()\n",
    "                if edge_data.op.get_embedded_ops():\n",
    "                    for primitive in edge_data.op.get_embedded_ops():\n",
    "                        primitive.parse()\n",
    "                model.add_module(\n",
    "                    \"{}-edge({},{})\".format(model.name, node_idx, neigbor_idx),\n",
    "                    edge_data.op,\n",
    "                )\n",
    "        #model.is_parsed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('naslib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbaa411444fd0f6a0219be7247cafe8232f9ca19b314371503f22ac5d4b08631"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
